{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Classification Report to Pandas Dataframe\n",
    "\n",
    "In this code I modify the code given by Sklearn in https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/metrics/classification.py#L1363 for returning a Pandas dataframe instead of a print of the classification report function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required pakages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classification_report(y_true, y_pred, labels = None, target_names=None,\n",
    "                             sample_weight=None, digits=2):\n",
    "    ''' \n",
    "    y_true = real classes\n",
    "    y_pred = predicted classes\n",
    "    labels = classes\n",
    "    '''\n",
    "    \n",
    "    #obtain the classes\n",
    "    if labels == None:\n",
    "        labels = np.unique(np.array(y_true))\n",
    "    else:\n",
    "        labels = np.sort(np.array(labels))\n",
    "        \n",
    "    \n",
    "    #define the colnames\n",
    "    colname = ['precision', 'recall', 'f1_score', 'support']\n",
    "    #define the rownames\n",
    "    rownames = np.array(labels).tolist()\n",
    "    rownames.append('avg/total')\n",
    "    \n",
    "    #calculate the values\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred,\n",
    "                                                  labels=labels,\n",
    "                                                  average=None,\n",
    "                                                  sample_weight=sample_weight)\n",
    "    \n",
    "    #compute the average\n",
    "    a = np.array([np.average(p, weights= s),\n",
    "                 np.average(r, weights= s),\n",
    "                 np.average(f1, weights= s),\n",
    "                 np.sum(s)])\n",
    "    \n",
    "    #now, fill the report with the obtained values\n",
    "    val = np.concatenate([[p],[r],[f1],[s]]).T\n",
    "    \n",
    "    #create the final matrix with all the values\n",
    "    ris = np.zeros((len(rownames), len(colname)))\n",
    "    ris[0:len(rownames) - 1,:] = val #insert the average / total\n",
    "    ris[len(rownames) - 1,:] = a #insert the average / total\n",
    "    \n",
    "    #round with digits\n",
    "    ris = np.round_(ris, decimals= digits)\n",
    "    ris[:,3] = np.round_(ris[:,3], decimals= 0)\n",
    "    \n",
    "    #define the empty dataframe\n",
    "    report = pd.DataFrame(ris,\n",
    "                         columns= colname,\n",
    "                         index= rownames)\n",
    "    \n",
    "    \n",
    "    return report\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision  recall  f1_score  support\n",
      "0                0.5    1.00      0.67      1.0\n",
      "1                0.0    0.00      0.00      1.0\n",
      "2                1.0    0.67      0.80      3.0\n",
      "avg/total        0.7    0.60      0.61      5.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      1.00      0.67         1\n",
      "          1       0.00      0.00      0.00         1\n",
      "          2       1.00      0.67      0.80         3\n",
      "\n",
      "avg / total       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###test 1\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "\n",
    "print(my_classification_report(y_true, y_pred, labels= [1,2,0]))\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision  recall  f1_score  support\n",
      "0                0.5     0.5      0.50      2.0\n",
      "1                0.5     1.0      0.67      1.0\n",
      "2                1.0     0.5      0.67      2.0\n",
      "avg/total        0.7     0.6      0.60      5.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.50      0.50         2\n",
      "          1       0.50      1.00      0.67         1\n",
      "          2       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.70      0.60      0.60         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [2, 2, 0, 1, 0]\n",
    "y_pred = [2, 0, 0, 1, 1]\n",
    "\n",
    "print(my_classification_report(y_true, y_pred, labels= [1,2,0]))\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
